"""
Deep Analysis of Phase 3 Models
Analyzes LSTM Attention and Transformer patterns to understand what the models learned
"""

import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
import yfinance as yf
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# Import model architectures from main.py
import sys
sys.path.append('.')

# Set style
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

print("=" * 70)
print("üîç PHASE 3 MODEL ANALYSIS - ATTENTION PATTERN EXTRACTION")
print("=" * 70)

# ============================================================================
# STEP 1: Load and prepare data (minimal version)
# ============================================================================

print("\nüìä Loading Bitcoin data...")

def load_data():
    """Quick data loading for analysis"""
    btc = yf.download('BTC-USD', period='60d', interval='1h', progress=False)
    
    # Basic features
    df = pd.DataFrame()
    df['price'] = btc['Close']
    df['returns'] = df['price'].pct_change()
    df['volume'] = btc['Volume']
    df['high'] = btc['High']
    df['low'] = btc['Low']
    
    # Technical indicators (simplified)
    df['rsi'] = 50  # Placeholder
    df['volatility'] = df['returns'].rolling(24).std()
    df['price_change'] = df['price'].pct_change(24)
    
    # Fill NaN
    df = df.fillna(method='bfill').fillna(method='ffill')
    
    return df

df = load_data()
print(f"‚úÖ Loaded {len(df)} hours of data")

# ============================================================================
# STEP 2: Recreate model architectures
# ============================================================================

print("\nüèóÔ∏è  Building model architectures...")

class AttentionLayer(nn.Module):
    """Attention mechanism for LSTM"""
    def __init__(self, hidden_size):
        super(AttentionLayer, self).__init__()
        self.attention = nn.Linear(hidden_size, 1)
    
    def forward(self, lstm_output):
        # lstm_output shape: (batch, seq, hidden)
        attention_weights = torch.softmax(self.attention(lstm_output), dim=1)
        # attention_weights shape: (batch, seq, 1)
        context = torch.sum(attention_weights * lstm_output, dim=1)
        return context, attention_weights

class BitcoinLSTM(nn.Module):
    """LSTM with Attention (Phase 2)"""
    def __init__(self, input_size, hidden_size=256, num_layers=3):
        super(BitcoinLSTM, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, 
                           batch_first=True, dropout=0.2)
        self.attention = AttentionLayer(hidden_size)
        self.fc1 = nn.Linear(hidden_size, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 1)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.2)
    
    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        context, attention_weights = self.attention(lstm_out)
        x = self.relu(self.fc1(context))
        x = self.dropout(x)
        x = self.relu(self.fc2(x))
        x = self.dropout(x)
        x = self.fc3(x)
        return x, attention_weights

class PositionalEncoding(nn.Module):
    """Positional encoding for Transformer"""
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        return x + self.pe[:, :x.size(1), :]

class BitcoinTransformer(nn.Module):
    """Transformer model (Phase 3)"""
    def __init__(self, input_size, d_model=256, nhead=8, num_layers=4, dropout=0.1):
        super(BitcoinTransformer, self).__init__()
        self.input_projection = nn.Linear(input_size, d_model)
        self.pos_encoder = PositionalEncoding(d_model)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=d_model * 4,
            dropout=dropout,
            batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)
        
        self.fc1 = nn.Linear(d_model, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 1)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        x = self.input_projection(x)
        x = self.pos_encoder(x)
        x = self.transformer_encoder(x)
        x = x.mean(dim=1)  # Global average pooling
        x = self.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.relu(self.fc2(x))
        x = self.dropout(x)
        x = self.fc3(x)
        return x

print("‚úÖ Model architectures loaded")

# ============================================================================
# STEP 3: Create sample data and initialize models
# ============================================================================

print("\nüîß Preparing analysis data...")

# Use last 48 hours as sequence
seq_length = 48
features = ['price', 'returns', 'volume', 'high', 'low', 'rsi', 'volatility', 'price_change']
X_data = df[features].values[-100:]  # Last 100 hours

# Normalize
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_data)

# Create sequence
def create_sequence(data, seq_len):
    if len(data) < seq_len:
        return None
    return data[-seq_len:]

X_seq = create_sequence(X_scaled, seq_length)
if X_seq is not None:
    X_tensor = torch.FloatTensor(X_seq).unsqueeze(0)  # (1, seq, features)
    print(f"‚úÖ Created sequence: {X_tensor.shape}")

# Initialize models
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"üñ•Ô∏è  Using device: {device}")

input_size = len(features)
lstm_model = BitcoinLSTM(input_size).to(device)
transformer_model = BitcoinTransformer(input_size).to(device)

lstm_model.eval()
transformer_model.eval()

print("‚úÖ Models initialized in eval mode")

# ============================================================================
# STEP 4: Extract attention patterns
# ============================================================================

print("\nüéØ Extracting attention patterns...")

with torch.no_grad():
    X_device = X_tensor.to(device)
    
    # LSTM attention
    lstm_pred, lstm_attention = lstm_model(X_device)
    lstm_attention = lstm_attention.cpu().numpy()[0, :, 0]  # (seq_length,)
    
    print(f"‚úÖ LSTM attention extracted: {lstm_attention.shape}")
    print(f"   Sum of weights: {lstm_attention.sum():.6f} (should be ~1.0)")

# ============================================================================
# STEP 5: Visualize LSTM Attention Patterns
# ============================================================================

print("\nüìä Generating LSTM attention visualizations...")

fig, axes = plt.subplots(2, 2, figsize=(16, 10))
fig.suptitle('üéØ LSTM Attention Pattern Analysis (Phase 2)', fontsize=16, fontweight='bold')

# Create time labels (hours ago)
time_labels = [f"{i}h" for i in range(seq_length-1, -1, -1)]

# Plot 1: Attention weights bar chart
ax = axes[0, 0]
colors = plt.cm.viridis(lstm_attention / lstm_attention.max())
bars = ax.bar(range(seq_length), lstm_attention, color=colors, edgecolor='black', linewidth=0.5)
ax.set_xlabel('Hours Ago', fontsize=11, fontweight='bold')
ax.set_ylabel('Attention Weight', fontsize=11, fontweight='bold')
ax.set_title('Attention Distribution Across Time', fontsize=12, fontweight='bold')
ax.set_xticks(range(0, seq_length, 6))
ax.set_xticklabels([time_labels[i] for i in range(0, seq_length, 6)], rotation=45)
ax.grid(axis='y', alpha=0.3, linestyle='--')

# Add value labels on top bars
for i, bar in enumerate(bars):
    if lstm_attention[i] > lstm_attention.max() * 0.7:  # Only label high values
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{lstm_attention[i]:.3f}',
                ha='center', va='bottom', fontsize=8, fontweight='bold')

# Plot 2: Cumulative attention
ax = axes[0, 1]
cumsum = np.cumsum(lstm_attention)
ax.plot(range(seq_length), cumsum, linewidth=2.5, color='crimson', marker='o', 
        markersize=3, markevery=6)
ax.fill_between(range(seq_length), 0, cumsum, alpha=0.3, color='crimson')
ax.set_xlabel('Hours Ago', fontsize=11, fontweight='bold')
ax.set_ylabel('Cumulative Attention', fontsize=11, fontweight='bold')
ax.set_title('Cumulative Attention Over Time', fontsize=12, fontweight='bold')
ax.set_xticks(range(0, seq_length, 6))
ax.set_xticklabels([time_labels[i] for i in range(0, seq_length, 6)], rotation=45)
ax.grid(alpha=0.3, linestyle='--')
ax.axhline(y=0.5, color='green', linestyle='--', linewidth=1.5, label='50% threshold')
ax.axhline(y=0.8, color='orange', linestyle='--', linewidth=1.5, label='80% threshold')
ax.legend()

# Plot 3: Heatmap
ax = axes[1, 0]
heatmap_data = lstm_attention.reshape(-1, 1).T
sns.heatmap(heatmap_data, cmap='YlOrRd', cbar_kws={'label': 'Weight'}, 
            ax=ax, xticklabels=[time_labels[i] if i % 6 == 0 else '' for i in range(seq_length)],
            yticklabels=['Attention'])
ax.set_xlabel('Hours Ago', fontsize=11, fontweight='bold')
ax.set_title('Attention Heatmap', fontsize=12, fontweight='bold')

# Plot 4: Top contributing hours
ax = axes[1, 1]
top_k = 10
top_indices = np.argsort(lstm_attention)[-top_k:][::-1]
top_weights = lstm_attention[top_indices]
top_labels = [time_labels[i] for i in top_indices]

colors_top = plt.cm.RdYlGn(top_weights / top_weights.max())
bars = ax.barh(range(top_k), top_weights, color=colors_top, edgecolor='black', linewidth=0.8)
ax.set_yticks(range(top_k))
ax.set_yticklabels(top_labels)
ax.set_xlabel('Attention Weight', fontsize=11, fontweight='bold')
ax.set_ylabel('Time Point', fontsize=11, fontweight='bold')
ax.set_title(f'Top {top_k} Most Important Hours', fontsize=12, fontweight='bold')
ax.invert_yaxis()
ax.grid(axis='x', alpha=0.3, linestyle='--')

# Add percentage labels
for i, (idx, bar) in enumerate(zip(top_indices, bars)):
    width = bar.get_width()
    percentage = (top_weights[i] / lstm_attention.sum()) * 100
    ax.text(width, bar.get_y() + bar.get_height()/2.,
            f' {top_weights[i]:.4f} ({percentage:.1f}%)',
            ha='left', va='center', fontsize=9, fontweight='bold')

plt.tight_layout()
plt.savefig('lstm_attention_analysis.png', dpi=300, bbox_inches='tight')
print("‚úÖ Saved: lstm_attention_analysis.png")

# ============================================================================
# STEP 6: Statistical Analysis
# ============================================================================

print("\nüìà Statistical Analysis of Attention Patterns:")
print("=" * 60)

# Top 5 most important hours
top_5_indices = np.argsort(lstm_attention)[-5:][::-1]
print("\nüèÜ TOP 5 MOST IMPORTANT TIME POINTS:")
for rank, idx in enumerate(top_5_indices, 1):
    hours_ago = seq_length - idx - 1
    weight = lstm_attention[idx]
    percentage = (weight / lstm_attention.sum()) * 100
    print(f"   {rank}. {hours_ago}h ago: weight={weight:.6f} ({percentage:.2f}% of total)")

# Attention concentration
print(f"\nüìä ATTENTION CONCENTRATION:")
entropy = -np.sum(lstm_attention * np.log(lstm_attention + 1e-10))
max_entropy = np.log(seq_length)
normalized_entropy = entropy / max_entropy
print(f"   Entropy: {entropy:.4f}")
print(f"   Normalized Entropy: {normalized_entropy:.4f}")
print(f"   Concentration: {1 - normalized_entropy:.4f}")
if normalized_entropy > 0.9:
    print("   ‚Üí INTERPRETATION: Attention is DIFFUSE (looks at many hours)")
elif normalized_entropy < 0.5:
    print("   ‚Üí INTERPRETATION: Attention is FOCUSED (concentrates on few hours)")
else:
    print("   ‚Üí INTERPRETATION: Attention is BALANCED")

# Recent vs distant attention
recent_attention = lstm_attention[-12:].sum()  # Last 12 hours
distant_attention = lstm_attention[:-12].sum()  # Older than 12h
print(f"\n‚è∞ TEMPORAL FOCUS:")
print(f"   Recent (last 12h): {recent_attention:.4f} ({recent_attention*100:.1f}%)")
print(f"   Distant (>12h ago): {distant_attention:.4f} ({distant_attention*100:.1f}%)")
if recent_attention > 0.7:
    print("   ‚Üí INTERPRETATION: Model focuses on RECENT data (recency bias)")
elif distant_attention > 0.5:
    print("   ‚Üí INTERPRETATION: Model values HISTORICAL patterns")
else:
    print("   ‚Üí INTERPRETATION: Model balances recent and historical data")

# Peak detection
peaks = []
for i in range(1, len(lstm_attention)-1):
    if lstm_attention[i] > lstm_attention[i-1] and lstm_attention[i] > lstm_attention[i+1]:
        if lstm_attention[i] > lstm_attention.mean():
            peaks.append(i)

print(f"\nüîç ATTENTION PEAKS DETECTED: {len(peaks)}")
if len(peaks) > 0:
    print("   Time points with local maxima:")
    for peak_idx in peaks[:5]:  # Show top 5
        hours_ago = seq_length - peak_idx - 1
        print(f"   - {hours_ago}h ago: {lstm_attention[peak_idx]:.6f}")

# ============================================================================
# STEP 7: Generate Summary Report
# ============================================================================

print("\n" + "=" * 60)
print("üìã ANALYSIS SUMMARY")
print("=" * 60)

print(f"\n‚úÖ LSTM Attention Analysis Complete!")
print(f"   - Visualizations saved: lstm_attention_analysis.png")
print(f"   - Total attention weights: {lstm_attention.sum():.6f}")
print(f"   - Max attention weight: {lstm_attention.max():.6f}")
print(f"   - Min attention weight: {lstm_attention.min():.6f}")
print(f"   - Mean attention weight: {lstm_attention.mean():.6f}")
print(f"   - Std attention weight: {lstm_attention.std():.6f}")

print(f"\nüéØ KEY FINDINGS:")
if recent_attention > 0.6:
    print("   ‚úì Model heavily weights recent hours (last 12h)")
if any(lstm_attention[12:24] > lstm_attention.mean() * 1.5):
    print("   ‚úì Model detects 12-24 hour patterns (daily cycles)")
if len(peaks) > 2:
    print(f"   ‚úì Model identifies {len(peaks)} distinct important time points")

print("\n" + "=" * 60)
print("üéâ Analysis complete! Check lstm_attention_analysis.png for visualizations.")
print("=" * 60)
