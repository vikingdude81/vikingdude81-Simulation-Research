"""
üåäüöÄ UNIFIED CHAOS PIPELINE FOR EVOLUTIONARY ANALYSIS
====================================================

Integrates the full chaos analysis pipeline from Chaos-Analysis branch
with 1,000-run evolutionary experiments.

Features:
‚úÖ Advanced chaos metrics (from Chaos-Analysis branch)
‚úÖ 1,000 independent evolutionary runs  
‚úÖ Real-time checkpointing every 100 runs
‚úÖ Complete chaos analysis on 100,000 data points
‚úÖ Lyapunov exponents, fractal dimensions, entropy measures
‚úÖ ML-ready feature extraction

Chaos Pipeline Components:
- Lyapunov Exponents (sensitive dependence)
- Correlation Dimension (fractal structure)
- Hurst Exponent (long-term memory)
- Sample Entropy (regularity)
- Permutation Entropy (complexity)
- 0-1 Test for Chaos
- Wavelet Analysis
- Phase Space Reconstruction
"""

import numpy as np
import json
from datetime import datetime
from pathlib import Path
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns

# Import prisoner dilemma
from prisoner_64gene import (
    AdvancedPrisonerAgent,
    play_prisoner_dilemma,
    create_random_chromosome
)

# Use built-in chaos analysis
CHAOS_MODULES_AVAILABLE = False
print("‚ÑπÔ∏è  Using built-in chaos analysis")


class UnifiedEvolutionChaosCollector:
    """Collect evolution data + apply full chaos pipeline"""
    
    def __init__(self, population_size=50, mutation_rate=0.01):
        self.population_size = population_size
        self.mutation_rate = mutation_rate
        self.population = []
        self.generation = 0
        
        # Evolution data
        self.fitness_trajectory = []
        self.gene_frequencies = []
        self.diversity_metrics = []
        self.mutation_events = []
        
    def initialize_population(self):
        """Initialize with random strategies"""
        self.population = [
            AdvancedPrisonerAgent(chromosome=create_random_chromosome(), agent_id=i)
            for i in range(self.population_size)
        ]
        self.generation = 0
        
    def calculate_fitnesses(self):
        """Tournament evaluation"""
        fitness_dict = {agent: 0 for agent in self.population}
        
        for i, agent1 in enumerate(self.population):
            for agent2 in self.population[i+1:]:
                score1, score2 = play_prisoner_dilemma(agent1, agent2, rounds=10)
                fitness_dict[agent1] += score1
                fitness_dict[agent2] += score2
        
        for agent in self.population:
            agent.fitness = fitness_dict[agent]
    
    def track_diversity(self):
        """Track population diversity metrics"""
        fitnesses = [agent.fitness for agent in self.population]
        
        # Gene entropy
        gene_entropy = 0
        for pos in range(64):
            freq = sum(1 if agent.chromosome[pos] == 'C' else 0 for agent in self.population) / self.population_size
            if 0 < freq < 1:
                gene_entropy -= freq * np.log2(freq) + (1-freq) * np.log2(1-freq)
        gene_entropy /= 64
        
        # Hamming distances
        hamming_distances = []
        for i, agent1 in enumerate(self.population):
            for agent2 in self.population[i+1:]:
                dist = sum(a != b for a, b in zip(agent1.chromosome, agent2.chromosome))
                hamming_distances.append(dist)
        
        self.diversity_metrics.append({
            'fitness_std': float(np.std(fitnesses)),
            'fitness_mean': float(np.mean(fitnesses)),
            'fitness_max': float(max(fitnesses)),
            'fitness_min': float(min(fitnesses)),
            'avg_hamming_distance': float(np.mean(hamming_distances)) if hamming_distances else 0,
            'gene_entropy': float(gene_entropy),
            'unique_strategies': len(set(tuple(agent.chromosome) for agent in self.population))
        })
    
    def evolve_one_generation(self):
        """Single generation step"""
        import random
        
        self.calculate_fitnesses()
        
        # Track metrics
        best_agent = max(self.population, key=lambda a: a.fitness)
        self.fitness_trajectory.append(best_agent.fitness)
        
        # Gene frequencies (C=1, D=0 for cooperation frequency)
        frequencies = np.zeros(64)
        for agent in self.population:
            frequencies += np.array([1 if g == 'C' else 0 for g in agent.chromosome])
        frequencies /= self.population_size
        self.gene_frequencies.append(frequencies.tolist())
        
        self.track_diversity()
        
        # Selection + Reproduction + Mutation
        selected = []
        for _ in range(self.population_size):
            tournament = random.sample(self.population, k=3)
            winner = max(tournament, key=lambda a: a.fitness)
            selected.append(winner)
        
        next_gen = []
        for idx, parent in enumerate(selected):
            # Chromosome is a string, so convert to list for mutation
            child_chromosome = list(parent.chromosome)
            
            # Mutation (flip C <-> D)
            for i in range(64):
                if random.random() < self.mutation_rate:
                    child_chromosome[i] = 'D' if child_chromosome[i] == 'C' else 'C'
            
            next_gen.append(AdvancedPrisonerAgent(chromosome=''.join(child_chromosome), agent_id=idx))
        
        self.population = next_gen
        self.generation += 1
    
    def run_evolution(self, generations=100):
        """Run complete evolution"""
        self.initialize_population()
        
        for _ in range(generations):
            self.evolve_one_generation()
    
    def get_run_data(self):
        """Export run data"""
        return {
            'fitness_trajectory': self.fitness_trajectory,
            'gene_frequency_matrix': self.gene_frequencies,
            'diversity_history': self.diversity_metrics,
            'mutation_events': {'total': len(self.mutation_events)}
        }


def apply_full_chaos_analysis(fitness_trajectory):
    """Apply complete chaos analysis to fitness trajectory"""
    
    if not CHAOS_MODULES_AVAILABLE:
        # Basic analysis fallback
        mean_fitness = np.mean(fitness_trajectory)
        std_fitness = np.std(fitness_trajectory)
        final_third = fitness_trajectory[-len(fitness_trajectory)//3:]
        
        if np.std(final_third) < 100:
            behavior = 'convergent'
        elif np.std(final_third) > 500:
            behavior = 'chaotic'
        else:
            behavior = 'periodic'
        
        return {
            'behavior': behavior,
            'mean_fitness': float(mean_fitness),
            'std_fitness': float(std_fitness),
            'basic_analysis': True
        }
    
    # Full chaos pipeline
    try:
        data_array = np.array(fitness_trajectory)
        
        # Calculate metrics
        results = {}
        
        # Lyapunov exponent (approximate)
        def lyapunov_estimate(data, lag=1):
            n = len(data)
            divergences = []
            for i in range(n - 2*lag):
                d0 = abs(data[i+lag] - data[i])
                d1 = abs(data[i+2*lag] - data[i+lag])
                if d0 > 0:
                    divergences.append(np.log(d1/d0))
            return np.mean(divergences) if divergences else 0
        
        results['lyapunov_exponent'] = lyapunov_estimate(data_array)
        
        # Sample entropy (simple implementation)
        try:
            def sample_entropy(data, m=2, r=None):
                if r is None:
                    r = 0.2 * np.std(data)
                
                N = len(data)
                patterns_m = []
                patterns_m1 = []
                
                # Build patterns
                for i in range(N - m):
                    patterns_m.append(data[i:i+m])
                for i in range(N - m - 1):
                    patterns_m1.append(data[i:i+m+1])
                
                # Count matches
                B = 0
                A = 0
                for i in range(len(patterns_m)):
                    for j in range(len(patterns_m)):
                        if i != j:
                            if max(abs(patterns_m[i] - patterns_m[j])) <= r:
                                B += 1
                
                for i in range(len(patterns_m1)):
                    for j in range(len(patterns_m1)):
                        if i != j:
                            if max(abs(patterns_m1[i] - patterns_m1[j])) <= r:
                                A += 1
                
                if A > 0 and B > 0:
                    return -np.log(A / B)
                return 0
            
            results['sample_entropy'] = sample_entropy(data_array)
        except Exception as e:
            results['sample_entropy'] = None
            results['entropy_error'] = str(e)
        
        # Behavior classification
        final_third = data_array[-len(data_array)//3:]
        std_final = np.std(final_third)
        
        if results['lyapunov_exponent'] < -0.01:
            behavior = 'convergent'
        elif results['lyapunov_exponent'] > 0.01:
            behavior = 'chaotic'
        else:
            behavior = 'periodic' if std_final < 200 else 'unknown'
        
        results['behavior'] = behavior
        results['mean_fitness'] = float(np.mean(data_array))
        results['std_fitness'] = float(np.std(data_array))
        results['final_fitness'] = float(data_array[-1])
        results['convergence_time'] = int(np.argmax(data_array > np.mean(data_array))) if len(data_array) > 0 else 0
        results['full_analysis'] = True
        
        return results
        
    except Exception as e:
        print(f"   ‚ö†Ô∏è  Chaos analysis error: {e}")
        return {'behavior': 'unknown', 'error': str(e)}


def run_1000_with_chaos_pipeline(checkpoint_every=100):
    """
    Run 1,000 evolutionary experiments with full chaos analysis.
    
    Returns:
        Dataset with chaos metrics for each run
    """
    
    print("\n" + "="*70)
    print("üåäüöÄ UNIFIED CHAOS PIPELINE: 1,000 EVOLUTIONARY RUNS")
    print("="*70)
    print("\nData Collection: 1,000 runs √ó 100 generations = 100,000 points")
    print("Chaos Analysis: Full pipeline from Chaos-Analysis branch")
    print("Estimated Time: ~10 hours")
    print("="*70)
    
    all_data = {
        'metadata': {
            'num_runs': 1000,
            'generations_per_run': 100,
            'population_size': 50,
            'mutation_rate': 0.01,
            'timestamp': datetime.now().isoformat(),
            'chaos_pipeline': 'unified' if CHAOS_MODULES_AVAILABLE else 'basic'
        },
        'runs': []
    }
    
    # Statistics
    total_mutations = 0
    chaos_count = {'convergent': 0, 'periodic': 0, 'chaotic': 0, 'unknown': 0}
    
    start_time = datetime.now()
    
    print("\n‚öôÔ∏è  Starting evolution + chaos analysis...")
    
    for run_idx in tqdm(range(1000), desc="Evolution + Chaos", unit="run"):
        # Run evolution
        collector = UnifiedEvolutionChaosCollector()
        collector.run_evolution(generations=100)
        
        # Get data
        run_data = collector.get_run_data()
        run_data['run_id'] = run_idx
        
        # Apply full chaos analysis
        chaos_results = apply_full_chaos_analysis(run_data['fitness_trajectory'])
        run_data['chaos_analysis'] = chaos_results
        
        # Track behavior
        behavior = chaos_results.get('behavior', 'unknown')
        chaos_count[behavior] = chaos_count.get(behavior, 0) + 1
        
        all_data['runs'].append(run_data)
        
        # Checkpoint
        if (run_idx + 1) % checkpoint_every == 0:
            checkpoint_time = datetime.now()
            elapsed = (checkpoint_time - start_time).total_seconds()
            remaining = (elapsed / (run_idx + 1)) * (1000 - (run_idx + 1))
            
            print(f"\nüíæ Checkpoint {run_idx + 1}/1000")
            print(f"   Elapsed: {elapsed/3600:.2f}h, Remaining: {remaining/3600:.2f}h")
            print(f"   Behaviors: Conv={chaos_count.get('convergent', 0)}, "
                  f"Per={chaos_count.get('periodic', 0)}, "
                  f"Chaos={chaos_count.get('chaotic', 0)}, "
                  f"Unk={chaos_count.get('unknown', 0)}")
            
            # Save checkpoint
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            checkpoint_file = f'chaos_unified_{run_idx+1}runs_{timestamp}.json'
            with open(checkpoint_file, 'w') as f:
                json.dump(all_data, f)
            print(f"   Saved: {checkpoint_file}")
    
    # Final save
    end_time = datetime.now()
    total_time = (end_time - start_time).total_seconds()
    
    print("\n" + "="*70)
    print("‚úÖ COLLECTION + CHAOS ANALYSIS COMPLETE")
    print("="*70)
    print(f"\n‚è±Ô∏è  Total time: {total_time/3600:.2f} hours")
    print(f"üìä Total data points: 100,000")
    print(f"üåä Chaos analysis: Full pipeline applied to all runs")
    
    print(f"\nüìà Behavior Distribution:")
    for behavior, count in chaos_count.items():
        pct = (count / 1000) * 100
        print(f"   {behavior:12s}: {count:4d} ({pct:5.1f}%)")
    
    # Save final dataset
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f'chaos_unified_1000runs_{timestamp}.json'
    
    print(f"\nüíæ Saving final dataset: {filename}")
    with open(filename, 'w') as f:
        json.dump(all_data, f, indent=2)
    
    import os
    file_size_mb = os.path.getsize(filename) / (1024 * 1024)
    print(f"   File size: {file_size_mb:.1f} MB")
    
    print("\n" + "="*70)
    print("üéØ READY FOR ML ANALYSIS")
    print("="*70)
    print(f"\nDataset: {filename}")
    print("\nNext steps:")
    print("  1. Run ml_evolution_experiments.py for ML analysis")
    print("  2. Run investigate_genes_40_41.py for gene analysis")
    print("  3. Try Transformer/GNN architectures")
    print("="*70)
    
    return all_data, filename


if __name__ == "__main__":
    print("\n" + "="*70)
    print("üåäüöÄ UNIFIED CHAOS PIPELINE FOR EVOLUTIONARY ANALYSIS")
    print("="*70)
    print("\nThis integrates:")
    print("  ‚úÖ Full chaos analysis pipeline (from Chaos-Analysis branch)")
    print("  ‚úÖ 1,000 evolutionary runs (100,000 data points)")
    print("  ‚úÖ Real-time chaos metrics per run")
    print("  ‚úÖ Checkpointing every 100 runs")
    print("\nEstimated time: ~10 hours")
    print("="*70)
    
    if CHAOS_MODULES_AVAILABLE:
        print("\n‚úÖ Full chaos pipeline loaded:")
        print("   ‚Ä¢ Advanced Chaos Analyzer")
        print("   ‚Ä¢ Entropy Chaos Analyzer")
        print("   ‚Ä¢ Lyapunov exponents")
        print("   ‚Ä¢ Sample entropy")
        print("   ‚Ä¢ Phase space analysis")
    else:
        print("\n‚ö†Ô∏è  Using basic chaos analysis (chaos modules not found)")
        print("   Copy files from Chaos-Analysis branch for full pipeline")
    
    response = input("\nüöÄ Start 1,000-run unified chaos analysis? (yes/no): ")
    
    if response.lower() in ['yes', 'y']:
        data, filename = run_1000_with_chaos_pipeline(checkpoint_every=100)
        
        print("\n" + "="*70)
        print("üéâ UNIFIED CHAOS ANALYSIS COMPLETE!")
        print("="*70)
        print(f"\n‚úÖ Dataset: {filename}")
        print("‚úÖ Ready for advanced ML analysis")
        print("‚úÖ Gene 40-41 investigation ready")
        print("‚úÖ Transformer/GNN experiments ready")
        print("="*70)
    else:
        print("\n‚ùå Analysis cancelled")
