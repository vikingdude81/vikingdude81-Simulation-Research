"""
üåäüöÄ LARGE-SCALE Chaos Data Collection (1,000 Runs)
===================================================

Generates 100,000 data points (1,000 runs √ó 100 generations) for:
‚úÖ Robust regime classification (convergent/periodic/chaotic)
‚úÖ Discovering rare chaotic regimes
‚úÖ High-confidence ML training
‚úÖ Gene importance analysis across diverse trajectories

ESTIMATED TIME: ~8 hours
DATASET SIZE: ~8 GB
"""

import numpy as np
import random
import json
from prisoner_64gene import (
    AdvancedPrisonerAgent, 
    play_prisoner_dilemma,
    create_random_chromosome,
    create_tit_for_tat
)
from datetime import datetime
from tqdm import tqdm

class ChaosDataCollector:
    """Collect extensive time-series data for chaos analysis"""
    
    def __init__(self, population_size=50, mutation_rate=0.01):
        self.population_size = population_size
        self.mutation_rate = mutation_rate
        self.population = []
        self.generation = 0
        
        # Data storage for chaos analysis
        self.fitness_time_series = []  # Best fitness per generation
        self.gene_frequencies = []  # 64-dim vector per generation
        self.diversity_metrics = []  # Various diversity measures
        self.strategy_embeddings = []  # PCA-like representation
        self.mutation_events = []  # Track all mutations
        self.interaction_matrix = []  # Pairwise game outcomes
        
    def initialize_population(self):
        """Initialize population with diverse random strategies"""
        self.population = [
            AdvancedPrisonerAgent(chromosome=create_random_chromosome())
            for _ in range(self.population_size)
        ]
        self.generation = 0
        
    def calculate_all_fitnesses(self):
        """Tournament-style fitness evaluation"""
        fitness_dict = {agent: 0 for agent in self.population}
        
        for i, agent1 in enumerate(self.population):
            for agent2 in self.population[i+1:]:
                score1, score2 = play_prisoner_dilemma(agent1, agent2, rounds=10)
                fitness_dict[agent1] += score1
                fitness_dict[agent2] += score2
        
        for agent in self.population:
            agent.fitness = fitness_dict[agent]
            
    def track_gene_frequencies(self):
        """Calculate frequency of '1' at each gene position"""
        frequencies = np.zeros(64)
        for agent in self.population:
            frequencies += np.array(agent.chromosome)
        frequencies /= self.population_size
        self.gene_frequencies.append(frequencies.tolist())
        
    def track_diversity(self):
        """Multiple diversity metrics"""
        fitnesses = [agent.fitness for agent in self.population]
        
        # Gene entropy (Shannon entropy across all 64 positions)
        gene_entropy = 0
        for pos in range(64):
            freq = sum(agent.chromosome[pos] for agent in self.population) / self.population_size
            if 0 < freq < 1:
                gene_entropy -= freq * np.log2(freq) + (1-freq) * np.log2(1-freq)
        gene_entropy /= 64  # Normalize
        
        # Hamming distance diversity
        hamming_distances = []
        for i, agent1 in enumerate(self.population):
            for agent2 in self.population[i+1:]:
                dist = sum(a != b for a, b in zip(agent1.chromosome, agent2.chromosome))
                hamming_distances.append(dist)
        
        self.diversity_metrics.append({
            'fitness_std': float(np.std(fitnesses)),
            'fitness_mean': float(np.mean(fitnesses)),
            'fitness_max': float(max(fitnesses)),
            'fitness_min': float(min(fitnesses)),
            'avg_hamming_distance': float(np.mean(hamming_distances)) if hamming_distances else 0,
            'gene_entropy': float(gene_entropy),
            'unique_strategies': len(set(tuple(agent.chromosome) for agent in self.population))
        })
        
    def track_mutations(self):
        """Track mutations in this generation"""
        mutation_count = 0
        positions_mutated = []
        
        for agent in self.population:
            # Simulation: each gene has mutation_rate chance
            for pos in range(64):
                if random.random() < self.mutation_rate:
                    mutation_count += 1
                    positions_mutated.append(pos)
        
        if mutation_count > 0:
            self.mutation_events.append({
                'generation': self.generation,
                'mutation_count': mutation_count,
                'positions': positions_mutated
            })
    
    def evolve_one_generation(self):
        """Single generation: selection + reproduction + mutation"""
        # Calculate fitness
        self.calculate_all_fitnesses()
        
        # Track metrics
        best_agent = max(self.population, key=lambda a: a.fitness)
        self.fitness_time_series.append(best_agent.fitness)
        self.track_gene_frequencies()
        self.track_diversity()
        
        # Selection (tournament)
        selected = []
        for _ in range(self.population_size):
            tournament = random.sample(self.population, k=3)
            winner = max(tournament, key=lambda a: a.fitness)
            selected.append(winner)
        
        # Reproduction + Mutation
        next_gen = []
        for parent in selected:
            child_chromosome = parent.chromosome.copy()
            
            # Mutation
            for i in range(64):
                if random.random() < self.mutation_rate:
                    child_chromosome[i] = 1 - child_chromosome[i]
            
            next_gen.append(AdvancedPrisonerAgent(chromosome=child_chromosome))
        
        self.population = next_gen
        self.generation += 1
        self.track_mutations()
    
    def run_evolution(self, generations=100):
        """Run evolution for N generations, collecting chaos data"""
        self.initialize_population()
        
        for _ in range(generations):
            self.evolve_one_generation()
    
    def get_run_data(self):
        """Export data from this run"""
        return {
            'fitness_trajectory': self.fitness_time_series,
            'gene_frequency_matrix': self.gene_frequencies,
            'diversity_history': self.diversity_metrics,
            'strategy_embeddings': self.strategy_embeddings,
            'mutation_events': {
                'total_mutations': sum(e['mutation_count'] for e in self.mutation_events),
                'mutations_per_generation': [e['mutation_count'] for e in self.mutation_events]
            }
        }

def run_large_scale_collection(num_runs=1000, generations_per_run=100, checkpoint_every=100):
    """
    Run 1,000 independent evolutionary experiments.
    
    Args:
        num_runs: Number of independent runs (default 1,000)
        generations_per_run: Generations per run (default 100)
        checkpoint_every: Save checkpoint every N runs (default 100)
    
    Returns:
        Complete dataset with 100,000 data points
    """
    print(f"\nüöÄ LARGE-SCALE DATA COLLECTION")
    print(f"   Runs: {num_runs:,}")
    print(f"   Generations per run: {generations_per_run}")
    print(f"   Total data points: {num_runs * generations_per_run:,}")
    print(f"   Estimated time: {num_runs * generations_per_run * 0.3 / 3600:.1f} hours")
    
    all_data = {
        'metadata': {
            'num_runs': num_runs,
            'generations_per_run': generations_per_run,
            'population_size': 50,
            'mutation_rate': 0.01,
            'timestamp': datetime.now().isoformat(),
            'total_generations': num_runs * generations_per_run
        },
        'runs': []
    }
    
    print("\n‚öôÔ∏è  Starting evolution runs...")
    
    # Track statistics
    total_mutations = 0
    converged_runs = 0
    diverged_runs = 0
    oscillating_runs = 0
    
    # Progress tracking
    start_time = datetime.now()
    
    for run_idx in tqdm(range(num_runs), desc="Evolution Runs", unit="run"):
        # Run evolution
        collector = ChaosDataCollector(population_size=50, mutation_rate=0.01)
        collector.run_evolution(generations=generations_per_run)
        
        # Collect data
        run_data = collector.get_run_data()
        run_data['run_id'] = run_idx
        all_data['runs'].append(run_data)
        
        # Track patterns
        total_mutations += run_data['mutation_events']['total_mutations']
        
        fitness_trajectory = np.array(run_data['fitness_trajectory'])
        final_third = fitness_trajectory[-generations_per_run//3:]
        
        if np.std(final_third) < 100:
            converged_runs += 1
        elif np.std(final_third) > 500:
            diverged_runs += 1
        else:
            oscillating_runs += 1
        
        # Checkpoint save
        if (run_idx + 1) % checkpoint_every == 0:
            checkpoint_time = datetime.now()
            elapsed = (checkpoint_time - start_time).total_seconds()
            remaining_runs = num_runs - (run_idx + 1)
            estimated_remaining = (elapsed / (run_idx + 1)) * remaining_runs
            
            print(f"\nüíæ Checkpoint at run {run_idx + 1}/{num_runs}")
            print(f"   Elapsed: {elapsed/3600:.2f} hours")
            print(f"   Estimated remaining: {estimated_remaining/3600:.2f} hours")
            print(f"   Converged: {converged_runs}/{run_idx+1} ({converged_runs/(run_idx+1)*100:.1f}%)")
            print(f"   Oscillating: {oscillating_runs}/{run_idx+1} ({oscillating_runs/(run_idx+1)*100:.1f}%)")
            print(f"   Diverged: {diverged_runs}/{run_idx+1} ({diverged_runs/(run_idx+1)*100:.1f}%)")
            
            # Save checkpoint
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            checkpoint_filename = f'chaos_dataset_checkpoint_{run_idx+1}runs_{timestamp}.json'
            with open(checkpoint_filename, 'w') as f:
                json.dump(all_data, f)
            print(f"   Saved: {checkpoint_filename}")
    
    # Final statistics
    end_time = datetime.now()
    total_time = (end_time - start_time).total_seconds()
    
    print("\n" + "="*70)
    print("‚úÖ COLLECTION COMPLETE")
    print("="*70)
    print(f"\n‚è±Ô∏è  Total time: {total_time/3600:.2f} hours")
    print(f"üìä Total data points: {num_runs * generations_per_run:,}")
    print(f"üß¨ Total gene measurements: {num_runs * generations_per_run * 64:,}")
    print(f"üîÑ Total mutations tracked: {total_mutations:,}")
    
    print(f"\nüìà Evolution patterns:")
    print(f"   - Converged: {converged_runs}/{num_runs} ({converged_runs/num_runs*100:.1f}%)")
    print(f"   - Oscillating: {oscillating_runs}/{num_runs} ({oscillating_runs/num_runs*100:.1f}%)")
    print(f"   - Diverged: {diverged_runs}/{num_runs} ({diverged_runs/num_runs*100:.1f}%)")
    
    # Save final dataset
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f'chaos_dataset_{num_runs}runs_{timestamp}.json'
    
    print(f"\nüíæ Saving final dataset: {filename}")
    with open(filename, 'w') as f:
        json.dump(all_data, f, indent=2)
    
    # Calculate file size
    import os
    file_size_mb = os.path.getsize(filename) / (1024 * 1024)
    print(f"   File size: {file_size_mb:.1f} MB")
    
    print("\n" + "="*70)
    print("üåä READY FOR ADVANCED CHAOS & ML ANALYSIS")
    print("="*70)
    print(f"\nDataset: {filename}")
    print("\n10√ó larger dataset enables:")
    print("  ‚úÖ Robust regime classification")
    print("  ‚úÖ Discovery of rare chaotic regimes")
    print("  ‚úÖ High-confidence ML predictions")
    print("  ‚úÖ Gene importance across diverse trajectories")
    print("  ‚úÖ Transformer & GNN architectures")
    print("="*70)
    
    return all_data, filename

if __name__ == "__main__":
    print("\n" + "="*70)
    print("üéØ LARGE-SCALE CHAOS DATA COLLECTION")
    print("="*70)
    print("\nThis will generate 100,000 data points (1,000 runs √ó 100 generations)")
    print("Estimated time: ~8 hours")
    print("\nProgress will be checkpointed every 100 runs.")
    print("\nData collected:")
    print("  ‚Ä¢ Fitness time series (1,000 trajectories)")
    print("  ‚Ä¢ 6,400,000 gene frequency measurements")
    print("  ‚Ä¢ Population diversity metrics")
    print("  ‚Ä¢ Mutation event tracking")
    print("\nEnables:")
    print("  ‚úÖ Robust ML classification (large training set)")
    print("  ‚úÖ Discovery of rare chaotic regimes")
    print("  ‚úÖ Gene importance analysis across diverse paths")
    print("  ‚úÖ Transformer & GNN architectures")
    print("="*70)
    
    response = input("\nüöÄ Start 1,000-run collection? (yes/no): ")
    
    if response.lower() in ['yes', 'y']:
        # Run large-scale experiment
        data, filename = run_large_scale_collection(num_runs=1000, generations_per_run=100, checkpoint_every=100)
        
        print("\n" + "="*70)
        print("üéâ LARGE-SCALE COLLECTION COMPLETE!")
        print("="*70)
        print(f"\nNext steps:")
        print(f"  1. Run chaos_analysis.py on '{filename}'")
        print(f"  2. Run ml_evolution_experiments.py for ML analysis")
        print(f"  3. Try Transformer/GNN architectures")
        print("="*70)
    else:
        print("\n‚ùå Collection cancelled.")
        print("   Run with 'python chaos_data_collection_1000runs.py' when ready.")
