{"file_contents":{"pyproject.toml":{"content":"[project]\nname = \"python-template\"\nversion = \"0.1.0\"\ndescription = \"\"\nauthors = [\"Your Name <you@example.com>\"]\nrequires-python = \">=3.11\"\ndependencies = [\n    \"matplotlib>=3.9.2\",\n    \"mplfinance>=0.12.10b0\",\n    \"numpy>=2.1.3\",\n    \"pandas>=2.2.3\",\n    \"pycoingecko>=3.2.0\",\n    \"requests>=2.32.3\",\n    \"scikit-learn>=1.5.2\",\n]\n","size_bytes":330},"main.py":{"content":"\nimport pandas as pd\nimport numpy as np\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport logging\n\n# Try to import LightGBM, but continue without it if unavailable\ntry:\n    import lightgbm as lgb\n    HAS_LIGHTGBM = True\nexcept (ImportError, OSError) as e:\n    logging.warning(f\"LightGBM not available: {e}\")\n    HAS_LIGHTGBM = False\n\n# Set up basic logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\n# --- CONFIGURATION ---\nFILE_PATH_90DAY = './DATA/BTC_90day_data.csv'\nFILE_PATH_1H = './attached_assets/COINBASE_BTCUSD, 60_5c089_1761289206450.csv'\nFILE_PATH_4H = './attached_assets/COINBASE_BTCUSD, 240_739cb_1761290157184.csv'\nFILE_PATH_12H = './attached_assets/COINBASE_BTCUSD, 720_c69cd_1761290157184.csv'\nFILE_PATH_1D = './attached_assets/COINBASE_BTCUSD, 1D_87ac3_1761289206450.csv'\nFILE_PATH_1W = './attached_assets/COINBASE_BTCUSD, 1W_9771c_1761290157184.csv'\n\n# Yahoo Finance data paths (fallback/supplement)\nYF_FILE_PATH_1H = './DATA/yf_btc_1h.csv'\nYF_FILE_PATH_4H = './DATA/yf_btc_4h.csv'\nYF_FILE_PATH_12H = './DATA/yf_btc_12h.csv'\nYF_FILE_PATH_1D = './DATA/yf_btc_1d.csv'\nYF_FILE_PATH_1W = './DATA/yf_btc_1w.csv'\n\nUSE_YAHOO_FINANCE = True  # Set to True to use YF data instead\nTEST_SIZE = 0.2\nPREDICT_STEPS = 12  # Predict next 12 hours\nUSE_ENSEMBLE = True  # Use ensemble of RF + XGBoost + LightGBM\n\n# Load and preprocess multi-timeframe data\ndef load_multi_timeframe_data():\n    \"\"\"Loads all timeframe data and returns them as dataframes.\"\"\"\n    try:\n        # Load 90-day baseline data (only needed for Coinbase data)\n        if not USE_YAHOO_FINANCE:\n            df_90day = pd.read_csv(FILE_PATH_90DAY)\n            df_90day['timestamp'] = pd.to_datetime(df_90day['timestamp'])\n            df_90day.set_index('timestamp', inplace=True)\n        else:\n            df_90day = None  # Not needed for Yahoo Finance\n        \n        if USE_YAHOO_FINANCE:\n            logging.info(\"Loading Yahoo Finance data...\")\n            \n            # Load Yahoo Finance data\n            df_1h = pd.read_csv(YF_FILE_PATH_1H)\n            df_1h['time'] = pd.to_datetime(df_1h['time'])\n            df_1h.set_index('time', inplace=True)\n            \n            df_4h = pd.read_csv(YF_FILE_PATH_4H)\n            df_4h['time'] = pd.to_datetime(df_4h['time'])\n            df_4h.set_index('time', inplace=True)\n            \n            df_12h = pd.read_csv(YF_FILE_PATH_12H)\n            df_12h['time'] = pd.to_datetime(df_12h['time'])\n            df_12h.set_index('time', inplace=True)\n            \n            df_1d = pd.read_csv(YF_FILE_PATH_1D)\n            df_1d['time'] = pd.to_datetime(df_1d['time'])\n            df_1d.set_index('time', inplace=True)\n            \n            df_1w = pd.read_csv(YF_FILE_PATH_1W)\n            df_1w['time'] = pd.to_datetime(df_1w['time'])\n            df_1w.set_index('time', inplace=True)\n            \n        else:\n            logging.info(\"Loading Coinbase indicator data...\")\n            \n            # Load 1-hour data with indicators\n            df_1h = pd.read_csv(FILE_PATH_1H)\n            df_1h['time'] = pd.to_datetime(df_1h['time'], unit='s')\n            df_1h.set_index('time', inplace=True)\n            \n            # Load 4-hour data with indicators\n            df_4h = pd.read_csv(FILE_PATH_4H)\n            df_4h['time'] = pd.to_datetime(df_4h['time'], unit='s')\n            df_4h.set_index('time', inplace=True)\n            \n            # Load 12-hour data with indicators\n            df_12h = pd.read_csv(FILE_PATH_12H)\n            df_12h['time'] = pd.to_datetime(df_12h['time'], unit='s')\n            df_12h.set_index('time', inplace=True)\n            \n            # Load 1-day data with indicators\n            df_1d = pd.read_csv(FILE_PATH_1D)\n            df_1d['time'] = pd.to_datetime(df_1d['time'], unit='s')\n            df_1d.set_index('time', inplace=True)\n            \n            # Load 1-week data with indicators\n            df_1w = pd.read_csv(FILE_PATH_1W)\n            df_1w['time'] = pd.to_datetime(df_1w['time'], unit='s')\n            df_1w.set_index('time', inplace=True)\n        \n        logging.info(\"All timeframe data loaded successfully\")\n        return df_90day, df_1h, df_4h, df_12h, df_1d, df_1w\n        \n    except FileNotFoundError as e:\n        logging.error(f\"File not found: {e}\")\n        raise\n    except Exception as e:\n        logging.error(f\"Error loading data: {e}\")\n        raise\n\ndef extract_indicator_features(df, prefix=''):\n    \"\"\"Extract key technical indicators from chart data.\"\"\"\n    features = {}\n    \n    # Price action\n    if 'close' in df.columns:\n        features[f'{prefix}close'] = df['close']\n        features[f'{prefix}price_change'] = df['close'].pct_change()\n    \n    # Bollinger Bands\n    if all(col in df.columns for col in ['BB Upper', 'BB Basis', 'BB Lower']):\n        features[f'{prefix}bb_width'] = (df['BB Upper'] - df['BB Lower']) / df['BB Basis']\n        features[f'{prefix}bb_position'] = (df['close'] - df['BB Lower']) / (df['BB Upper'] - df['BB Lower'])\n    \n    # Volume indicators\n    if 'Volume Band (Close)' in df.columns:\n        features[f'{prefix}volume'] = df['Volume Band (Close)']\n    \n    # Momentum indicators\n    if 'QAO' in df.columns:\n        features[f'{prefix}qao'] = df['QAO']\n    \n    # Z-scores for overbought/oversold\n    if all(col in df.columns for col in ['z20', 'z50', 'z100']):\n        features[f'{prefix}z20'] = df['z20']\n        features[f'{prefix}z50'] = df['z50']\n        features[f'{prefix}z100'] = df['z100']\n    \n    # Composite Z\n    if 'Composite Z (smoothed)' in df.columns:\n        features[f'{prefix}composite_z'] = df['Composite Z (smoothed)']\n    \n    # EMA trends\n    if all(col in df.columns for col in ['GMA fast', 'GMA slow']):\n        features[f'{prefix}ema_fast'] = df['GMA fast']\n        features[f'{prefix}ema_slow'] = df['GMA slow']\n        features[f'{prefix}ema_cross'] = df['GMA fast'] - df['GMA slow']\n    \n    # Volatility\n    if 'volatility' in df.columns:\n        features[f'{prefix}volatility'] = df['volatility']\n    elif 'close' in df.columns:\n        features[f'{prefix}volatility'] = df['close'].pct_change().rolling(window=7).std()\n    \n    return pd.DataFrame(features)\n\ndef combine_multi_timeframe_features(df_90day, df_1h, df_4h, df_12h, df_1d, df_1w):\n    \"\"\"Combine features from all timeframes using forward-fill for alignment.\"\"\"\n    \n    if df_90day is not None:\n        logging.info(f\"90-day data shape: {df_90day.shape}, date range: {df_90day.index.min()} to {df_90day.index.max()}\")\n    logging.info(f\"1h data shape: {df_1h.shape}, date range: {df_1h.index.min()} to {df_1h.index.max()}\")\n    logging.info(f\"4h data shape: {df_4h.shape}, date range: {df_4h.index.min()} to {df_4h.index.max()}\")\n    logging.info(f\"12h data shape: {df_12h.shape}, date range: {df_12h.index.min()} to {df_12h.index.max()}\")\n    logging.info(f\"1d data shape: {df_1d.shape}, date range: {df_1d.index.min()} to {df_1d.index.max()}\")\n    logging.info(f\"1w data shape: {df_1w.shape}, date range: {df_1w.index.min()} to {df_1w.index.max()}\")\n    \n    # Use 1-hour data as the base timeline for maximum training samples\n    combined = df_1h[['close']].copy()\n    combined.rename(columns={'close': 'price'}, inplace=True)\n    \n    # Extract features from each timeframe\n    features_1h = extract_indicator_features(df_1h, prefix='1h_')\n    features_4h = extract_indicator_features(df_4h, prefix='4h_')\n    features_12h = extract_indicator_features(df_12h, prefix='12h_')\n    features_1d = extract_indicator_features(df_1d, prefix='1d_')\n    features_1w = extract_indicator_features(df_1w, prefix='1w_')\n    \n    # 1h features already match the base timeframe\n    features_1h_resampled = features_1h\n    \n    # Resample 4h to hourly with forward fill (limit to 4 hours)\n    features_4h_resampled = features_4h.resample('1h').last()\n    features_4h_resampled = features_4h_resampled.reindex(combined.index)\n    features_4h_resampled = features_4h_resampled.ffill(limit=4)\n    \n    # Resample 12h to hourly with forward fill (limit to 12 hours)\n    features_12h_resampled = features_12h.resample('1h').last()\n    features_12h_resampled = features_12h_resampled.reindex(combined.index)\n    features_12h_resampled = features_12h_resampled.ffill(limit=12)\n    \n    # Resample daily to hourly with forward fill (limit to 24 hours)\n    features_1d_resampled = features_1d.resample('1h').last()\n    features_1d_resampled = features_1d_resampled.reindex(combined.index)\n    features_1d_resampled = features_1d_resampled.ffill(limit=24)\n    \n    # Resample weekly to hourly with forward fill (limit to 168 hours = 1 week)\n    features_1w_resampled = features_1w.resample('1h').last()\n    features_1w_resampled = features_1w_resampled.reindex(combined.index)\n    features_1w_resampled = features_1w_resampled.ffill(limit=168)\n    \n    # Combine all features\n    combined = pd.concat([combined, features_1h_resampled, features_4h_resampled, \n                         features_12h_resampled, features_1d_resampled, features_1w_resampled], axis=1)\n    \n    # Add original features from 90-day data\n    combined['pct_change'] = combined['price'].pct_change()\n    combined['rolling_mean_5'] = combined['price'].rolling(window=5).mean()\n    combined['rolling_std_5'] = combined['price'].rolling(window=5).std()\n    combined['rolling_mean_20'] = combined['price'].rolling(window=20).mean()\n    combined['rolling_std_20'] = combined['price'].rolling(window=20).std()\n    \n    # Target variable: predict percentage return instead of absolute price\n    combined['target_return'] = combined['price'].pct_change().shift(-1)\n    combined['next_price'] = combined['price'].shift(-1)  # Keep for reference\n    \n    logging.info(f\"Combined features before cleanup: {combined.shape}\")\n    logging.info(f\"NaN counts per column:\\n{combined.isna().sum()}\")\n    \n    # Drop rows where target_return is NaN (last row)\n    combined = combined[combined['target_return'].notna()]\n    \n    # Fill NaN in non-critical features with 0\n    combined = combined.fillna(0)\n    \n    logging.info(f\"Combined features shape after cleanup: {combined.shape}\")\n    logging.info(f\"Remaining NaN counts: {combined.isna().sum().sum()}\")\n    \n    return combined\n\ndef prepare_data(combined_df):\n    \"\"\"Prepare X and y from combined dataframe.\"\"\"\n    \n    # Select features (exclude price, target, and next_price reference)\n    feature_cols = [col for col in combined_df.columns if col not in ['price', 'target_return', 'next_price']]\n    \n    X = combined_df[feature_cols].values\n    y = combined_df['target_return'].values\n    \n    logging.info(f\"Feature count: {len(feature_cols)}\")\n    logging.info(f\"Sample count: {len(X)}\")\n    \n    return combined_df, X, y, feature_cols\n\ndef train_model(X_train, y_train, model_name='SVR'):\n    \"\"\"Train model with GridSearchCV.\"\"\"\n    if model_name == 'SVR':\n        model = SVR()\n        param_grid = {\n            'C': [0.1, 1, 10],\n            'gamma': ['scale', 0.1, 1],\n            'kernel': ['rbf']\n        }\n    elif model_name == 'RF':\n        model = RandomForestRegressor(random_state=42)\n        param_grid = {\n            'n_estimators': [50, 100, 200],\n            'max_depth': [10, 20, 30]\n        }\n    elif model_name == 'XGB':\n        model = xgb.XGBRegressor(random_state=42, tree_method='hist')\n        param_grid = {\n            'n_estimators': [100, 200],\n            'max_depth': [5, 10, 15],\n            'learning_rate': [0.01, 0.1]\n        }\n    elif model_name == 'LGB':\n        model = lgb.LGBMRegressor(random_state=42, verbose=-1)\n        param_grid = {\n            'n_estimators': [100, 200],\n            'max_depth': [5, 10, 15],\n            'learning_rate': [0.01, 0.1]\n        }\n    else:\n        raise ValueError(f\"Unknown model: {model_name}\")\n    \n    logging.info(f\"Starting GridSearchCV for {model_name}...\")\n    grid_search = GridSearchCV(model, param_grid, cv=3, scoring='neg_mean_squared_error', verbose=0, n_jobs=-1)\n    grid_search.fit(X_train, y_train)\n    \n    logging.info(f\"Best {model_name} Parameters: {grid_search.best_params_}\")\n    return grid_search.best_estimator_\n\ndef train_ensemble(X_train, y_train):\n    \"\"\"Train ensemble of RF, XGBoost, and optionally LightGBM models.\"\"\"\n    models = {}\n    \n    logging.info(\"Training RandomForest model...\")\n    models['RF'] = train_model(X_train, y_train, 'RF')\n    \n    logging.info(\"Training XGBoost model...\")\n    models['XGB'] = train_model(X_train, y_train, 'XGB')\n    \n    if HAS_LIGHTGBM:\n        logging.info(\"Training LightGBM model...\")\n        models['LGB'] = train_model(X_train, y_train, 'LGB')\n    else:\n        logging.info(\"LightGBM not available, using RF + XGBoost ensemble\")\n    \n    logging.info(f\"Ensemble training complete with {len(models)} models!\")\n    return models\n\ndef ensemble_predict(models, X):\n    \"\"\"Make predictions using ensemble of models with equal weighting.\"\"\"\n    predictions = []\n    \n    for name, model in models.items():\n        pred = model.predict(X)\n        predictions.append(pred)\n    \n    # Average predictions from all models\n    ensemble_pred = np.mean(predictions, axis=0)\n    return ensemble_pred\n\ndef predict_next_steps(model, df_last_row_full, scaler, features, steps=3):\n    \"\"\"Multi-step prediction carrying forward multi-timeframe features.\n    \n    Args:\n        model: Either a single model or dict of models (ensemble)\n        df_last_row_full: Last row of data with all features\n        scaler: Fitted StandardScaler\n        features: List of feature names\n        steps: Number of steps to predict\n    \"\"\"\n    predictions = []\n    prices = [df_last_row_full['price']]\n    current_features = df_last_row_full.copy()\n    \n    # Check if using ensemble\n    is_ensemble = isinstance(model, dict)\n    \n    for i in range(steps):\n        # Build feature vector from current state\n        feature_vector = []\n        for feat in features:\n            if feat in current_features.index:\n                feature_vector.append(current_features[feat])\n            else:\n                feature_vector.append(0)\n        \n        X_pred = np.array(feature_vector).reshape(1, -1)\n        X_pred_scaled = scaler.transform(X_pred)\n        \n        # Predict percentage return using ensemble or single model\n        if is_ensemble:\n            predicted_return = ensemble_predict(model, X_pred_scaled)[0]\n        else:\n            predicted_return = model.predict(X_pred_scaled)[0]\n        \n        # Convert return to price\n        current_price = prices[-1]\n        predicted_price = current_price * (1 + predicted_return)\n        prices.append(predicted_price)\n        predictions.append(predicted_price)\n        \n        # Update only price-dependent features for next iteration\n        # Keep multi-timeframe features constant (carry forward from last actual data)\n        current_features['price'] = predicted_price\n        \n        # Recalculate simple rolling features based on price history\n        price_series = pd.Series(prices[-21:])  # Last 20 + current\n        current_features['pct_change'] = predicted_return\n        if len(prices) >= 5:\n            current_features['rolling_mean_5'] = price_series[-5:].mean()\n            current_features['rolling_std_5'] = price_series[-5:].std()\n        if len(prices) >= 20:\n            current_features['rolling_mean_20'] = price_series[-20:].mean()\n            current_features['rolling_std_20'] = price_series[-20:].std()\n    \n    # Return predictions as a series with timestamps\n    last_timestamp = df_last_row_full.name\n    prediction_df = pd.DataFrame({\n        'price': predictions\n    }, index=[last_timestamp + pd.Timedelta(hours=i+1) for i in range(steps)])\n    \n    return predictions, prediction_df\n\n# Main execution\nif __name__ == \"__main__\":\n    try:\n        # Load all timeframe data\n        df_90day, df_1h, df_4h, df_12h, df_1d, df_1w = load_multi_timeframe_data()\n        \n        # Combine features from all timeframes\n        combined_df = combine_multi_timeframe_features(df_90day, df_1h, df_4h, df_12h, df_1d, df_1w)\n        \n        # Prepare data\n        df_final, X, y, features = prepare_data(combined_df)\n        \n        # Time series split\n        n_samples = len(X)\n        split_point = int(n_samples * (1 - TEST_SIZE))\n        \n        X_train = X[:split_point]\n        X_test = X[split_point:]\n        y_train = y[:split_point]\n        y_test = y[split_point:]\n        \n        logging.info(f\"Training set size: {len(X_train)}, Test set size: {len(X_test)}\")\n        \n        # Scale data\n        scaler = StandardScaler()\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_test_scaled = scaler.transform(X_test)\n        \n        # Train model(s)\n        if USE_ENSEMBLE:\n            logging.info(\"=\"*50)\n            model_list = \"RF + XGBoost + LightGBM\" if HAS_LIGHTGBM else \"RF + XGBoost\"\n            logging.info(f\"Training Ensemble ({model_list})\")\n            logging.info(\"=\"*50)\n            model = train_ensemble(X_train_scaled, y_train)\n            \n            # Evaluate ensemble\n            y_pred = ensemble_predict(model, X_test_scaled)\n            \n            # Individual model performance\n            logging.info(\"\\nIndividual Model Performance:\")\n            for name, m in model.items():\n                y_pred_individual = m.predict(X_test_scaled)\n                mse_individual = mean_squared_error(y_test, y_pred_individual)\n                logging.info(f\"  {name} RMSE: {np.sqrt(mse_individual):.6f}\")\n        else:\n            # Train single model - RF handles complex features better than SVR\n            model = train_model(X_train_scaled, y_train, model_name='RF')\n            y_pred = model.predict(X_test_scaled)\n        \n        # Ensemble/final model evaluation\n        mse = mean_squared_error(y_test, y_pred)\n        logging.info(f\"\\nEnsemble Test MSE: {mse:.4f}\")\n        logging.info(f\"Ensemble Test RMSE: {np.sqrt(mse):.4f}\")\n        \n        # Predict next steps using the full last row with all features\n        last_row_full = df_final.iloc[-1]\n        predictions, prediction_df = predict_next_steps(\n            model=model,\n            df_last_row_full=last_row_full,\n            scaler=scaler,\n            features=features,\n            steps=PREDICT_STEPS\n        )\n        \n        # Convert RMSE back to price scale (since we're predicting returns)\n        avg_price = df_final['price'].mean()\n        price_rmse = np.sqrt(mse) * avg_price\n        \n        # Display results\n        print(\"\\n\" + \"=\"*60)\n        if USE_ENSEMBLE:\n            print(\"ðŸŽ¯ ENSEMBLE MODEL SUMMARY (RF + XGBoost + LightGBM)\")\n        else:\n            print(\"MULTI-TIMEFRAME MODEL SUMMARY\")\n        print(\"=\"*60)\n        print(f\"Total Features Used: {len(features)}\")\n        print(f\"Training Samples: {len(X_train):,}\")\n        print(f\"Test Samples: {len(X_test):,}\")\n        \n        if USE_ENSEMBLE:\n            num_models = 3 if HAS_LIGHTGBM else 2\n            print(f\"Model: Ensemble ({num_models} models with equal weighting)\")\n            print(f\"  - RandomForest\")\n            print(f\"  - XGBoost\")\n            if HAS_LIGHTGBM:\n                print(f\"  - LightGBM\")\n        else:\n            print(f\"Model: RandomForest\")\n        \n        print(f\"\\nTest Set Return RMSE: {np.sqrt(mse):.6f} ({np.sqrt(mse)*100:.2f}%)\")\n        print(f\"Approximate Price RMSE: ${price_rmse:.2f}\")\n        print(f\"\\n--- Last Actual Price ---\")\n        print(f\"{last_row_full.name.strftime('%Y-%m-%d %H:%M')}: ${last_row_full['price']:.2f}\")\n        print(f\"\\n--- Predicted Prices for Next {PREDICT_STEPS} Hours ---\")\n        for date, row in prediction_df.iterrows():\n            print(f\"{date.strftime('%Y-%m-%d %H:%M')}: ${row['price']:.2f}\")\n        print(\"\\n--- Last 7 Data Points (Actual) ---\")\n        print(df_final[['price']].tail(7))\n        print(\"=\"*60)\n        \n    except Exception as e:\n        logging.error(f\"An error occurred: {e}\")\n        import traceback\n        traceback.print_exc()\n","size_bytes":20161}},"version":2}